{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGity5+H1vuCp41GL+za0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colesmcintosh/entropy-injection-cot/blob/main/Chain_Of_Thought_via_Entropy_Injection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxfyxmiC8zMy",
        "outputId": "39845021-afa3-4ce8-f084-85e6ae52cc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ga1xGXRi9gqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name\n",
        "model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
        "\n",
        "# Hugging Face authentication token\n",
        "# Ensure you have accepted the license and have your token ready\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "\n",
        "# Create a BitsAndBytesConfig object for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Load the model with 8-bit precision to save memory\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    use_auth_token=hf_token,\n",
        "    device_map='auto',\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bohmR_dl9lIH",
        "outputId": "46dbb5d7-5874-441a-f6b1-7e33be8a0cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(logits):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "omCwoy1E-G_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(logits, temperature=1.0, top_p=0.9):\n",
        "    logits = logits.squeeze(0)\n",
        "    logits = logits / temperature\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "    sorted_indices_to_remove[0] = False\n",
        "\n",
        "    sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
        "    probabilities = torch.softmax(sorted_logits, dim=-1)\n",
        "    next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "    next_token_id = sorted_indices[next_token]\n",
        "    return next_token_id.item()\n"
      ],
      "metadata": {
        "id": "RIiK0_DlBhUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy_based_cot_injection_with_logging(prompt, entropy_threshold=4.0, max_length=150, max_cot_injections=1, cooldown_steps=5):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    entropies = []\n",
        "    tokens_generated = []\n",
        "    cot_injections = 0\n",
        "    steps_since_cot = cooldown_steps  # Initialize to cooldown to allow immediate injection if needed\n",
        "\n",
        "    # Use a more guiding CoT prompt\n",
        "    cot_prompt = \" To determine the answer, let's breakdown the problem step by step, then provide a final answer. \"\n",
        "    cot_ids = tokenizer.encode(cot_prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_length):\n",
        "            # Prepare the model input\n",
        "            if cot_injections > 0 and steps_since_cot <= cooldown_steps:\n",
        "                # Include the CoT prompt in the model input but not in generated_ids\n",
        "                model_input_ids = torch.cat((input_ids, cot_ids, generated_ids[:, input_ids.size(1):]), dim=1)\n",
        "                attention_mask = torch.ones_like(model_input_ids)\n",
        "            else:\n",
        "                model_input_ids = generated_ids\n",
        "                attention_mask = torch.ones_like(model_input_ids)\n",
        "\n",
        "            outputs = model(input_ids=model_input_ids, attention_mask=attention_mask)\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropy = calculate_entropy(next_token_logits)\n",
        "            entropies.append(entropy.item())\n",
        "\n",
        "            # Check entropy threshold and cooldown\n",
        "            if (\n",
        "                entropy.item() > entropy_threshold\n",
        "                and cot_injections < max_cot_injections\n",
        "                and steps_since_cot >= cooldown_steps\n",
        "            ):\n",
        "                print(f\"Step {step+1}: Entropy {entropy.item():.4f} exceeds threshold. Injecting CoT prompt.\")\n",
        "                cot_injections += 1\n",
        "                steps_since_cot = 0\n",
        "                continue  # Recalculate after injecting CoT\n",
        "\n",
        "            # Generate next token\n",
        "            next_token_id = sample_next_token(next_token_logits)\n",
        "            next_token_id_tensor = torch.tensor([[next_token_id]], device=model.device)\n",
        "\n",
        "            # Append the token to the generated sequence\n",
        "            generated_ids = torch.cat((generated_ids, next_token_id_tensor), dim=1)\n",
        "            tokens_generated.append(next_token_id)\n",
        "\n",
        "            # Decode token for logging\n",
        "            token_str = tokenizer.decode([next_token_id])\n",
        "            print(f\"Step {step+1}: Generated token: '{token_str}' | Entropy: {entropy.item():.4f}\")\n",
        "\n",
        "            # Check for end-of-text token\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                print(f\"Step {step+1}: End-of-text token generated. Stopping generation.\")\n",
        "                break\n",
        "\n",
        "            steps_since_cot += 1\n",
        "\n",
        "        # Decode the generated text, excluding the CoT prompt\n",
        "        output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        return output_text, entropies, tokens_generated\n"
      ],
      "metadata": {
        "id": "XEWAjk54-H9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial prompt\n",
        "prompt = \"Is 9.9 greater than 9.11?\"\n",
        "\n",
        "# Run the entropy-based CoT injection\n",
        "output = entropy_based_cot_injection_with_logging(prompt)\n",
        "\n",
        "print(\"Output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrSejlry-M2V",
        "outputId": "ca7b6416-39d2-4921-c870-4818d68c49bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Generated token: ' Yes' | Entropy: 3.3201\n",
            "Step 2: Generated token: ',' | Entropy: 1.4193\n",
            "Step 3: Generated token: ' or' | Entropy: 2.1664\n",
            "Step 4: Generated token: ' no' | Entropy: 0.9418\n",
            "Step 5: Generated token: '.\n",
            "\n",
            "' | Entropy: 1.5586\n",
            "Step 6: Generated token: '##' | Entropy: 1.0052\n",
            "Step 7: Generated token: ' Step' | Entropy: 0.0001\n",
            "Step 8: Generated token: ' ' | Entropy: 0.0006\n",
            "Step 9: Generated token: '1' | Entropy: 0.0011\n",
            "Step 10: Generated token: ':' | Entropy: 0.0829\n",
            "Step 11: Generated token: ' Determine' | Entropy: 2.3941\n",
            "Step 12: Generated token: ' the' | Entropy: 0.4219\n",
            "Step 13: Generated token: ' comparison' | Entropy: 3.6358\n",
            "Step 14: Generated token: ' we' | Entropy: 2.6477\n",
            "Step 15: Generated token: ' need' | Entropy: 0.6533\n",
            "Step 16: Generated token: ' to' | Entropy: 0.0155\n",
            "Step 17: Generated token: ' make' | Entropy: 0.0506\n",
            "Step 18: Generated token: '.\n",
            "' | Entropy: 0.7651\n",
            "Step 19: Generated token: 'To' | Entropy: 0.4037\n",
            "Step 20: Generated token: ' compare' | Entropy: 1.6686\n",
            "Step 21: Generated token: ' ' | Entropy: 0.7217\n",
            "Step 22: Generated token: '9' | Entropy: 0.0017\n",
            "Step 23: Generated token: '.' | Entropy: 0.0002\n",
            "Step 24: Generated token: '9' | Entropy: 0.0273\n",
            "Step 25: Generated token: ' and' | Entropy: 0.8537\n",
            "Step 26: Generated token: ' ' | Entropy: 0.0001\n",
            "Step 27: Generated token: '9' | Entropy: 0.0000\n",
            "Step 28: Generated token: '.' | Entropy: 0.0000\n",
            "Step 29: Generated token: '11' | Entropy: 0.0003\n",
            "Step 30: Generated token: ',' | Entropy: 0.1468\n",
            "Step 31: Generated token: ' we' | Entropy: 0.0455\n",
            "Step 32: Generated token: ' need' | Entropy: 0.6247\n",
            "Step 33: Generated token: ' to' | Entropy: 0.0031\n",
            "Step 34: Generated token: ' find' | Entropy: 2.5139\n",
            "Step 35: Generated token: ' out' | Entropy: 0.6069\n",
            "Step 36: Generated token: ' if' | Entropy: 0.3914\n",
            "Step 37: Generated token: ' ' | Entropy: 0.9788\n",
            "Step 38: Generated token: '9' | Entropy: 0.0487\n",
            "Step 39: Generated token: '.' | Entropy: 0.0024\n",
            "Step 40: Generated token: '9' | Entropy: 0.1925\n",
            "Step 41: Generated token: ' is' | Entropy: 0.0473\n",
            "Step 42: Generated token: ' greater' | Entropy: 0.3393\n",
            "Step 43: Generated token: ' than' | Entropy: 0.2460\n",
            "Step 44: Generated token: ' ' | Entropy: 0.6425\n",
            "Step 45: Generated token: '9' | Entropy: 0.0004\n",
            "Step 46: Generated token: '.' | Entropy: 0.0010\n",
            "Step 47: Generated token: '11' | Entropy: 0.0017\n",
            "Step 48: Generated token: '.\n",
            "\n",
            "' | Entropy: 0.3591\n",
            "Step 49: Generated token: '##' | Entropy: 0.0005\n",
            "Step 50: Generated token: ' Step' | Entropy: 0.0000\n",
            "Step 51: Generated token: ' ' | Entropy: 0.0000\n",
            "Step 52: Generated token: '2' | Entropy: 0.0000\n",
            "Step 53: Generated token: ':' | Entropy: 0.0001\n",
            "Step 54: Generated token: ' Compare' | Entropy: 3.0720\n",
            "Step 55: Generated token: ' the' | Entropy: 0.4714\n",
            "Step 56: Generated token: ' numbers' | Entropy: 1.4323\n",
            "Step 57: Generated token: '.\n",
            "' | Entropy: 1.0642\n",
            "Step 58: Generated token: 'We' | Entropy: 1.5855\n",
            "Step 59: Generated token: ' compare' | Entropy: 1.8354\n",
            "Step 60: Generated token: ' ' | Entropy: 0.9115\n",
            "Step 61: Generated token: '9' | Entropy: 0.0469\n",
            "Step 62: Generated token: '.' | Entropy: 0.0222\n",
            "Step 63: Generated token: '9' | Entropy: 0.1320\n",
            "Step 64: Generated token: ' and' | Entropy: 1.4656\n",
            "Step 65: Generated token: ' ' | Entropy: 0.0044\n",
            "Step 66: Generated token: '9' | Entropy: 0.0027\n",
            "Step 67: Generated token: '.' | Entropy: 0.0009\n",
            "Step 68: Generated token: '11' | Entropy: 0.0061\n",
            "Step 69: Generated token: ' to' | Entropy: 2.0683\n",
            "Step 70: Generated token: ' see' | Entropy: 0.7788\n",
            "Step 71: Generated token: ' if' | Entropy: 0.7203\n",
            "Step 72: Generated token: ' ' | Entropy: 0.9996\n",
            "Step 73: Generated token: '9' | Entropy: 0.0288\n",
            "Step 74: Generated token: '.' | Entropy: 0.0091\n",
            "Step 75: Generated token: '9' | Entropy: 0.1621\n",
            "Step 76: Generated token: ' is' | Entropy: 0.4108\n",
            "Step 77: Generated token: ' greater' | Entropy: 1.0293\n",
            "Step 78: Generated token: ' than' | Entropy: 0.8287\n",
            "Step 79: Generated token: ' ' | Entropy: 0.0923\n",
            "Step 80: Generated token: '9' | Entropy: 0.0010\n",
            "Step 81: Generated token: '.' | Entropy: 0.0010\n",
            "Step 82: Generated token: '11' | Entropy: 0.0108\n",
            "Step 83: Generated token: '.' | Entropy: 0.6868\n",
            "Step 84: Generated token: ' Since' | Entropy: 2.2242\n",
            "Step 85: Generated token: ' ' | Entropy: 1.2474\n",
            "Step 86: Generated token: '9' | Entropy: 0.5698\n",
            "Step 87: Generated token: '.' | Entropy: 0.2071\n",
            "Step 88: Generated token: '9' | Entropy: 0.4891\n",
            "Step 89: Generated token: ' is' | Entropy: 1.1836\n",
            "Step 90: Generated token: ' the' | Entropy: 2.2792\n",
            "Step 91: Generated token: ' same' | Entropy: 2.4160\n",
            "Step 92: Generated token: ' as' | Entropy: 0.8524\n",
            "Step 93: Generated token: ' ' | Entropy: 0.8643\n",
            "Step 94: Generated token: '9' | Entropy: 0.7031\n",
            "Step 95: Generated token: '.' | Entropy: 1.6486\n",
            "Step 96: Generated token: '990' | Entropy: 2.4659\n",
            "Step 97: Generated token: ' and' | Entropy: 2.5245\n",
            "Step 98: Generated token: ' ' | Entropy: 1.9860\n",
            "Step 99: Generated token: '9' | Entropy: 0.1427\n",
            "Step 100: Generated token: '.' | Entropy: 0.0095\n",
            "Step 101: Generated token: '11' | Entropy: 0.5852\n",
            "Step 102: Generated token: ' is' | Entropy: 0.5268\n",
            "Step 103: Generated token: ' the' | Entropy: 1.8811\n",
            "Step 104: Generated token: ' same' | Entropy: 0.1064\n",
            "Step 105: Generated token: ' as' | Entropy: 0.0633\n",
            "Step 106: Generated token: ' ' | Entropy: 0.0260\n",
            "Step 107: Generated token: '9' | Entropy: 0.0067\n",
            "Step 108: Generated token: '.' | Entropy: 0.0016\n",
            "Step 109: Generated token: '119' | Entropy: 1.0943\n",
            "Step 110: Generated token: '9' | Entropy: 2.1546\n",
            "Step 111: Generated token: ' (' | Entropy: 1.2505\n",
            "Step 112: Entropy 4.9825 exceeds threshold. Injecting CoT prompt.\n",
            "Step 113: Generated token: 'two' | Entropy: 5.1392\n",
            "Step 114: Generated token: ' decimal' | Entropy: 0.4430\n",
            "Step 115: Generated token: ' places' | Entropy: 0.1972\n",
            "Step 116: Generated token: ' less' | Entropy: 2.2344\n",
            "Step 117: Generated token: '),' | Entropy: 1.5829\n",
            "Step 118: Generated token: ' it' | Entropy: 2.3390\n",
            "Step 119: Generated token: ' is' | Entropy: 1.8835\n",
            "Step 120: Generated token: ' clear' | Entropy: 2.0897\n",
            "Step 121: Generated token: ' that' | Entropy: 0.4098\n",
            "Step 122: Generated token: ' ' | Entropy: 0.4014\n",
            "Step 123: Generated token: '9' | Entropy: 0.0485\n",
            "Step 124: Generated token: '.' | Entropy: 0.0016\n",
            "Step 125: Generated token: '9' | Entropy: 1.2229\n",
            "Output:\n",
            "('Is 9.9 greater than 9.11? Yes, or no.\\n\\n## Step 1: Determine the comparison we need to make.\\nTo compare 9.9 and 9.11, we need to find out if 9.9 is greater than 9.11.\\n\\n## Step 2: Compare the numbers.\\nWe compare 9.9 and 9.11 to see if 9.9 is greater than 9.11. Since 9.9 is the same as 9.990 and 9.11 is the same as 9.1199 (two decimal places less), it is clear that 9.9', [3.320124626159668, 1.4193353652954102, 2.166351795196533, 0.9418296813964844, 1.558639645576477, 1.0052129030227661, 0.00012836346286348999, 0.0005703220376744866, 0.0011418701615184546, 0.08289281278848648, 2.394092559814453, 0.4219297170639038, 3.6357522010803223, 2.6476871967315674, 0.6533089280128479, 0.015546200796961784, 0.0506121888756752, 0.7651247978210449, 0.4037294387817383, 1.6685798168182373, 0.7216557264328003, 0.0016994269099086523, 0.00020863753161393106, 0.02727309986948967, 0.8536874055862427, 0.00013474462321028113, 4.338238068157807e-05, 1.1803587767644785e-05, 0.00031751967617310584, 0.1468464583158493, 0.04548506811261177, 0.6246517896652222, 0.00306326593272388, 2.513890027999878, 0.6069235801696777, 0.3914073705673218, 0.9788098335266113, 0.048735130578279495, 0.0024297849740833044, 0.19254988431930542, 0.047308001667261124, 0.3393002450466156, 0.24603629112243652, 0.6425451040267944, 0.0003939839079976082, 0.0010488976258784533, 0.0016659318935126066, 0.3590637743473053, 0.0005011674948036671, 1.877098111435771e-05, 9.285353371524252e-06, 3.494683141980204e-06, 6.0904429119545966e-05, 3.072049140930176, 0.47140973806381226, 1.4323151111602783, 1.0641978979110718, 1.585503339767456, 1.8354465961456299, 0.9114593267440796, 0.04693746566772461, 0.022153310477733612, 0.13201309740543365, 1.4656200408935547, 0.00443394435569644, 0.0026595185045152903, 0.0008714968571439385, 0.006074770353734493, 2.0682694911956787, 0.7787728905677795, 0.7202929854393005, 0.9996267557144165, 0.028803706169128418, 0.009122170507907867, 0.1621232032775879, 0.41080960631370544, 1.029339075088501, 0.8287062644958496, 0.09226468205451965, 0.001047779805958271, 0.0009531823452562094, 0.010788945481181145, 0.6868091821670532, 2.2241902351379395, 1.247426986694336, 0.5697720050811768, 0.20714439451694489, 0.48907309770584106, 1.183554768562317, 2.27915620803833, 2.4160447120666504, 0.8524308204650879, 0.8642585277557373, 0.7030701041221619, 1.6486080884933472, 2.465902090072632, 2.5244698524475098, 1.9859790802001953, 0.14266549050807953, 0.009501518681645393, 0.5852086544036865, 0.5268346071243286, 1.8811002969741821, 0.10642795264720917, 0.06330668181180954, 0.026028050109744072, 0.006711888127028942, 0.001593773253262043, 1.0943143367767334, 2.154634475708008, 1.2505322694778442, 4.98248291015625, 5.139194488525391, 0.4430214762687683, 0.1971982717514038, 2.2343907356262207, 1.5828609466552734, 2.338972806930542, 1.8834507465362549, 2.089704990386963, 0.40984421968460083, 0.4013928771018982, 0.048547253012657166, 0.0016212890623137355, 1.222886085510254], [7566, 11, 477, 912, 382, 567, 15166, 220, 16, 25, 31001, 279, 12593, 584, 1205, 311, 1304, 627, 1271, 9616, 220, 24, 13, 24, 323, 220, 24, 13, 806, 11, 584, 1205, 311, 1505, 704, 422, 220, 24, 13, 24, 374, 7191, 1109, 220, 24, 13, 806, 382, 567, 15166, 220, 17, 25, 24702, 279, 5219, 627, 1687, 9616, 220, 24, 13, 24, 323, 220, 24, 13, 806, 311, 1518, 422, 220, 24, 13, 24, 374, 7191, 1109, 220, 24, 13, 806, 13, 8876, 220, 24, 13, 24, 374, 279, 1890, 439, 220, 24, 13, 19146, 323, 220, 24, 13, 806, 374, 279, 1890, 439, 220, 24, 13, 9079, 24, 320, 20375, 12395, 7634, 2753, 705, 433, 374, 2867, 430, 220, 24, 13, 24])\n"
          ]
        }
      ]
    }
  ]
}